## Visualization

* Olah, et al., "The Building Blocks of Interpretability", Distill, 2018. https://distill.pub/2018/building-blocks/
* Visualizing Representations: Deep Learning and Human Beings, Colah's Blog, http://colah.github.io/posts/2015-01-Visualizing-Representations/
* Opening the Black Box of Deep Neural Networks via Information, Ravid Shwartz-Ziv, Naftali Tishby. [arXiv:1703.00810](https://arxiv.org/abs/1703.00810)
* Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. [arXiv:1312.6034](https://arxiv.org/abs/1312.6034)

## Sensitivity and Relevance Analysis
* Explaining NonLinear Classification Decisions with Deep Taylor Decomposition, Grégoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, Klaus-Robert Müller. [arXiv:1512.02479](https://arxiv.org/abs/1512.02479)


# Conferences

* [NeurIPS2018](https://nips.cc/Conferences/2018)
  * *Workshop* [Causal Learning](https://sites.google.com/view/nips2018causallearning/home)
  * *Workshop* [Interpretability and Robustness in Audio, Speech, and Language](https://irasl.gitlab.io/)
